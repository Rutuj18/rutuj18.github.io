<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>How does ChatGPT know what word comes next?</title>

  <!-- MathJax for rendering math -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">

  <style>
    body {
      font-family: 'Inter', sans-serif;
      margin: 0 auto;
      padding: 2rem;
      max-width: 750px;
      background: #fefefe;
      color: #222;
      line-height: 1.7;
    }

    h1, h2, h3 {
      color: #1a1a1a;
    }

    h1 {
      font-size: 2.2rem;
      margin-bottom: 0.8rem;
    }

    h2 {
      font-size: 1.5rem;
      margin-top: 2rem;
      margin-bottom: 0.6rem;
      border-bottom: 2px solid #e2e2e2;
      padding-bottom: 0.3rem;
    }

    hr {
      margin: 2rem 0;
      border: none;
      border-top: 1px solid #ddd;
    }

    pre {
      background: #f0f0f0;
      padding: 1rem;
      border-radius: 6px;
      overflow-x: auto;
    }

    code {
      font-family: monospace;
      background-color: #f7f7f7;
      padding: 2px 5px;
      border-radius: 4px;
    }

    ul, ol {
      margin-left: 1.2rem;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
      font-size: 0.95rem;
    }

    th, td {
      border: 1px solid #ccc;
      padding: 8px;
    }

    th {
      background: #f3f3f3;
      font-weight: 600;
    }

    a {
      color: #0077cc;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .footer {
      margin-top: 4rem;
      font-size: 0.9rem;
      color: #666;
    }
  </style>
</head>
<body>

  <h1>How does ChatGPT know what word comes next? It’s not magic — it’s discrete math</h1>

  <p>When you're typing and ChatGPT seems to psychically predict your next word, it can feel like magic. But it’s not. Underneath the surface, this behavior is powered by beautifully structured, surprisingly simple mathematics — the kind that falls squarely in the realm of discrete math. In this blog, we’ll take a closer look at the structures and processes that make this possible: tokenization, prediction via graphs, and attention mechanisms. All powered by finite sets, probabilities, and graphs — not spells.</p>

  <hr>

  <h2>Tokenization — Words Are Not Words</h2>

  <p>Before ChatGPT can predict the next word, it needs to break input into chunks it understands. These are not necessarily words, but <strong>tokens</strong> — subword units generated using an algorithm called <strong>Byte Pair Encoding (BPE)</strong> [1][2].</p>

  <p>Byte Pair Encoding starts with individual characters and repeatedly merges the most frequent adjacent pairs. For example, the string <code>aaabdaaabac</code> might evolve like this:</p>

  <ul>
    <li>Step 1: Replace most frequent pair <code>aa</code> → <code>Z</code>: <code>ZabdZabac</code></li>
    <li>Step 2: Replace <code>ab</code> → <code>Y</code>: <code>ZYdZYac</code></li>
  </ul>

  <p>This continues until the vocabulary reaches a predefined size.</p>

  <p>This process can be represented using <strong>tries</strong> (prefix trees) or <strong>finite automata</strong>. In terms of discrete math, tokenization translates raw continuous text into a <strong>discrete sequence over a finite alphabet</strong>, creating a structured input for the model.</p>

  <hr>

  <h2>Prediction = Probabilities on Graphs</h2>

  <p>Once the input is tokenized, GPT's job is to predict the <strong>next</strong> token. A basic model of this behavior can be described using <strong>Markov chains</strong>, where the next state depends only on the current one.</p>

  <p>The prediction process can be visualized using a <strong>directed weighted graph</strong>:</p>

  <ul>
    <li><strong>Nodes</strong> represent tokens</li>
    <li><strong>Edges</strong> represent the probability of transitioning from one token to another</li>
    <li>The edge from token A to token B has weight P(B | A)</li>
  </ul>

  <p>For example, a small Markov model might look like this:</p>

  <pre>
START → quick (0.5), brown (0.5)
quick → fox (1.0)
brown → fox (1.0)
fox → END (1.0)
  </pre>

  <p>The probability of a sequence is computed by multiplying transition probabilities:</p>

  <p>$$
  P(t_0, t_1, ..., t_k) = P(t_0) \cdot P(t_1|t_0) \cdot \dots \cdot P(t_k|t_{k-1})
  $$</p>

  <hr>

  <h2>Attention is Selection — Not All Tokens Matter</h2>

  <p>Traditional Markov models are limited by the assumption that the next state depends only on the current one. GPT-like models break this limitation using <strong>self-attention</strong>, which allows each token to consider <em>all</em> previous tokens.</p>

  <p>Each token computes a relevance score with every other token using dot products. These scores are normalized (via softmax), producing weights that determine which tokens to focus on.</p>

  <p>This mechanism, while computed in real numbers, operates over <strong>discrete index sets</strong> — selecting a subset of tokens. In discrete math terms:</p>

  <ul>
    <li>A <strong>combinatorial selection problem</strong>: which tokens to attend to?</li>
    <li>A generalization of <strong>Markov chains</strong>: attention considers multiple states weighted by importance.</li>
  </ul>

  <p>Recent theoretical work shows that transformer attention can be modeled as <strong>context-conditioned Markov chains (CCMC)</strong> [3].</p>

  <hr>

  <h2>Why This Isn’t Magic (But Feels Like It)</h2>

  <p>GPT predicts text by chaining together token predictions. Each step involves:</p>

  <ol>
    <li>Reading the current sequence of tokens</li>
    <li>Computing probabilities for the next token</li>
    <li>Sampling or choosing the most likely one</li>
    <li>Repeating</li>
  </ol>

  <p>Though executed with massive neural layers, this process rests on a foundation of <strong>discrete math</strong>:</p>

  <ul>
    <li>Finite alphabets</li>
    <li>Graph-based models</li>
    <li>Probability over discrete spaces</li>
    <li>Combinatorics and selection</li>
  </ul>

  <hr>

  <h2>Try It Yourself — A Mini Markov Chain</h2>

  <pre>
START → quick (0.5), brown (0.5)
quick → fox (1.0)
brown → fox (1.0)
fox → END (1.0)
  </pre>

  <p>Two possible sequences:</p>
  <ol>
    <li>START → quick → fox → END</li>
    <li>START → brown → fox → END</li>
  </ol>

  <p>Each with equal probability — a simple example of GPT’s core logic.</p>

  <hr>

  <h2>Summary Table</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Discrete Math Structure</th>
        <th>Role in ChatGPT</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Tokenization</td>
        <td>DFA / Trie</td>
        <td>Converts text into a token sequence</td>
      </tr>
      <tr>
        <td>Next-token prediction</td>
        <td>Directed weighted graph (Markov)</td>
        <td>Predicts the next token</td>
      </tr>
      <tr>
        <td>Attention</td>
        <td>Context-conditioned transitions</td>
        <td>Selectively conditions on previous tokens</td>
      </tr>
    </tbody>
  </table>

  <hr>

  <h2>References</h2>

  <ol>
    <li>GeeksforGeeks. Byte Pair Encoding (BPE) in NLP. <a href="https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/">Link</a></li>
    <li>Wikipedia. Byte Pair Encoding. <a href="https://en.wikipedia.org/wiki/Byte-pair_encoding">Link</a></li>
    <li>Arxiv. Mapping Transformers to Context-Conditioned Markov Chains. <a href="https://arxiv.org/pdf/2402.13512">Link</a></li>
  </ol>

  <div class="footer">
    © 2025 · Rutuja Swami · Summer of Math Exposition Entry
  </div>
</body>
</html>
