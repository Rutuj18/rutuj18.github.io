<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>How does ChatGPT know what word comes next?</title>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">

  <!-- MathJax for equations -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script async id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- vis.js for graph -->
  <script type="text/javascript" src="https://unpkg.com/vis-network@9.1.2/dist/vis-network.min.js"></script>
  <link href="https://unpkg.com/vis-network@9.1.2/dist/vis-network.min.css" rel="stylesheet" />

  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      background: linear-gradient(145deg, #f6f3ff, #e3f4ff);
      min-height: 100vh;
      color: #222;
      display: flex;
      justify-content: center;
      padding: 2rem 1rem;
    }

    .container {
      background-color: white;
      padding: 2.5rem 2rem;
      border-radius: 20px;
      max-width: 820px;
      width: 100%;
      box-shadow: 0 12px 30px rgba(0, 0, 0, 0.08);
    }

    h1 {
      font-size: 2.2rem;
      color: #3a0ca3;
      margin-bottom: 1rem;
    }

    h2 {
      color: #4361ee;
      border-bottom: 2px solid #e1e1e1;
      padding-bottom: 0.4rem;
      margin-top: 2.2rem;
    }

    p, li {
      line-height: 1.7;
      font-size: 1.05rem;
    }

    pre {
      background-color: #f5f7ff;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
    }

    code {
      background-color: #f0f0ff;
      padding: 2px 5px;
      border-radius: 4px;
      font-family: monospace;
    }

    button {
      margin: 0.5rem 0.3rem;
      padding: 0.4rem 0.9rem;
      font-size: 0.95rem;
      border: none;
      border-radius: 6px;
      cursor: pointer;
    }

    .step-btn {
      background: #4361ee;
      color: white;
    }

    .reset-btn {
      background: #e63946;
      color: white;
    }

    #markov-graph {
      height: 300px;
      margin: 1rem 0;
      border: 1px solid #ccc;
      border-radius: 12px;
    }

    table {
      border-collapse: collapse;
      margin-top: 1rem;
      width: 100%;
    }

    th, td {
      border: 1px solid #ccc;
      padding: 10px;
    }

    th {
      background-color: #f0f4ff;
    }

    .footer {
      margin-top: 3rem;
      font-size: 0.9rem;
      color: #555;
      text-align: center;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>How does ChatGPT know what word comes next? Itâ€™s not magic â€” itâ€™s discrete math</h1>

    <p>When you're typing and ChatGPT seems to psychically predict your next word, it can feel like magic. But itâ€™s not. Underneath the surface, this behavior is powered by beautifully structured, surprisingly simple mathematics â€” the kind that falls squarely in the realm of discrete math. In this blog, weâ€™ll take a closer look at the structures and processes that make this possible: tokenization, prediction via graphs, and attention mechanisms. All powered by finite sets, probabilities, and graphs â€” not spells.</p>

    <h2>Tokenization â€” Words Are Not Words</h2>
    <p>Before ChatGPT can predict the next word, it needs to break input into chunks it understands. These are not necessarily words, but <strong>tokens</strong> â€” subword units generated using an algorithm called <strong>Byte Pair Encoding (BPE)</strong> [1][2].</p>

    <p>Byte Pair Encoding starts with individual characters and repeatedly merges the most frequent adjacent pairs. For example, the string <code>aaabdaaabac</code> might evolve like this:</p>
    <ul>
      <li>Step 1: Replace most frequent pair <code>aa</code> â†’ <code>Z</code>: <code>ZabdZabac</code></li>
      <li>Step 2: Replace <code>ab</code> â†’ <code>Y</code>: <code>ZYdZYac</code></li>
    </ul>
    <p>This continues until the vocabulary reaches a predefined size.</p>

    <p>This process can be represented using <strong>tries</strong> (prefix trees) or <strong>finite automata</strong>. In discrete math terms, tokenization turns raw text into a <strong>discrete sequence over a finite alphabet</strong>.</p>

    <h2>Prediction = Probabilities on Graphs</h2>
    <p>Once input is tokenized, GPT predicts the <strong>next</strong> token. A simplified version of this uses a <strong>Markov chain</strong>, where the next token depends only on the current one.</p>

    <p>The process can be visualized as a <strong>directed weighted graph</strong> where:</p>
    <ul>
      <li><strong>Nodes</strong> = tokens</li>
      <li><strong>Edges</strong> = transition probabilities between tokens</li>
    </ul>

    <p>Example graph:</p>
    <pre>
START â†’ quick (0.5), brown (0.5)
quick â†’ fox (1.0)
brown â†’ fox (1.0)
fox â†’ END (1.0)
    </pre>

    <p>The probability of a sequence is:</p>
    <p>$$P(t_0, ..., t_k) = P(t_0) \cdot P(t_1|t_0) \cdot \dots \cdot P(t_k|t_{k-1})$$</p>

    <h2>Try It Yourself â€” Interactive Markov Model</h2>
    <p>Press <strong>Step</strong> to simulate how GPT predicts tokens one-by-one on this toy Markov chain!</p>
    <div id="markov-graph"></div>
    <p><strong>Current Path:</strong> <span id="markov-path">START</span></p>
    <button class="step-btn" onclick="stepMarkov()">â–¶ Step</button>
    <button class="reset-btn" onclick="resetMarkov()">ðŸ”„ Reset</button>

    <script>
      const nodes = new vis.DataSet([
        { id: "START", label: "START", color: "#48cae4" },
        { id: "quick", label: "quick" },
        { id: "brown", label: "brown" },
        { id: "fox", label: "fox" },
        { id: "END", label: "END", color: "#ade8f4" }
      ]);

      const edges = new vis.DataSet([
        { from: "START", to: "quick", label: "0.5" },
        { from: "START", to: "brown", label: "0.5" },
        { from: "quick", to: "fox", label: "1.0" },
        { from: "brown", to: "fox", label: "1.0" },
        { from: "fox", to: "END", label: "1.0" }
      ]);

      const container = document.getElementById("markov-graph");
      const data = { nodes: nodes, edges: edges };
      const options = { physics: false, edges: { arrows: 'to' } };
      const network = new vis.Network(container, data, options);

      const transitions = {
        "START": [["quick", 0.5], ["brown", 0.5]],
        "quick": [["fox", 1.0]],
        "brown": [["fox", 1.0]],
        "fox": [["END", 1.0]],
        "END": []
      };

      let current = "START";
      let path = ["START"];

      function stepMarkov() {
        if (current === "END") return;
        let options = transitions[current];
        let rand = Math.random();
        let total = 0;
        for (let [next, prob] of options) {
          total += prob;
          if (rand <= total) {
            current = next;
            path.push(next);
            document.getElementById("markov-path").textContent = path.join(" â†’ ");
            break;
          }
        }
      }

      function resetMarkov() {
        current = "START";
        path = ["START"];
        document.getElementById("markov-path").textContent = "START";
      }
    </script>

    <h2>Attention is Selection â€” Not All Tokens Matter</h2>
    <p>Markov chains only look at the immediate past. GPT uses <strong>self-attention</strong>, letting each token consider <em>all</em> previous tokens.</p>
    <p>This is computed via dot products and softmax â€” but conceptually, it's about selecting which previous tokens are important. In discrete math terms:</p>
    <ul>
      <li><strong>Selection over sets</strong> â€” combinatorics</li>
      <li><strong>Generalized transitions</strong> â€” context-aware Markov chains [3]</li>
    </ul>

    <h2>Summary Table</h2>
    <table>
      <tr><th>Concept</th><th>Discrete Math</th><th>Use in GPT</th></tr>
      <tr><td>Tokenization</td><td>Tries, Finite Automata</td><td>Break text into tokens</td></tr>
      <tr><td>Prediction</td><td>Markov Chains</td><td>Next-token probabilities</td></tr>
      <tr><td>Attention</td><td>Set selection, Context-aware chains</td><td>Weighs all prior tokens</td></tr>
    </table>

    <h2>References</h2>
    <ol>
      <li><a href="https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/" target="_blank">Byte Pair Encoding â€“ GeeksForGeeks</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Byte-pair_encoding" target="_blank">Byte Pair Encoding â€“ Wikipedia</a></li>
      <li><a href="https://arxiv.org/pdf/2402.13512" target="_blank">Context-Conditioned Markov Chains â€“ arXiv</a></li>
    </ol>

    <div class="footer">
      Â© 2025 Â· Rutuja Swami Â· Summer of Math Exposition Blog ðŸŒ¸
    </div>
  </div>
</body>
</html>
